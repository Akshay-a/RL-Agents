{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6457a6",
   "metadata": {},
   "source": [
    "#two networks need to be trained in this ALgorithm , typical actor critic framework\n",
    "# first network is policy network which basically outputs the probability distribution of all different set of actions in the env\n",
    "#next is the value network function which will return   value which basically suggests how good the current postion is\n",
    "\n",
    "# Actor Network\n",
    "input_layer → hidden_layers → output_layer(softmax for discrete actions)\n",
    "\n",
    "# Critic Network  \n",
    "input_layer → hidden_layers → output_layer(single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82794adc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#below is sample training loop, need to understand a bit more\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Data Collection\n",
    "    collect_trajectories()\n",
    "    \n",
    "    # 2. Advantage Calculation\n",
    "    compute_advantages_and_returns()\n",
    "    \n",
    "    # 3. Policy Update (multiple times)\n",
    "    for _ in range(policy_updates):\n",
    "        update_actor()\n",
    "        if kl_divergence > threshold:\n",
    "            break  # Early stopping\n",
    "    \n",
    "    # 4. Value Function Update\n",
    "    for _ in range(value_updates):\n",
    "        update_critic()\n",
    "\"\"\"\n",
    "#below is the policy gradient theorum and is foundation for PPO and later we add kinda clipping to it.   \n",
    "\"\"\"\n",
    "# Basic policy gradient (REINFORCE)\n",
    "for episode in episodes:\n",
    "    actions, states, rewards = collect_episode()\n",
    "    returns = compute_returns(rewards)  # Future rewards\n",
    "    \n",
    "    for t in range(len(actions)):\n",
    "        loss = -log_prob(actions[t]) * returns[t]\n",
    "        loss.backward()  # Update policy to increase good actions\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e713d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#dry run for REINFORCE Algorithm ( simpple loss fxn )\n",
    "#THIS CELL IS TO TRY OUT IMPLEMENTING REINFORCE ALGORITHM FIRST TO GET USED TO POLICY GRADIENT METHODS, NEXT ONWARDS WILL TRY RUNNING ppo WITH GAE Advantage function\n",
    "import gymnasium as gym # Use gymnasium instead of gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "        return x\n",
    "\n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "\n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R\n",
    "            loss = -torch.log(prob) * R\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    pi = Policy()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "\n",
    "    for n_epi in range(2500): # Reduced episodes for demonstration\n",
    "        s, info = env.reset() # gymnasium's reset returns observation and info\n",
    "        done = False\n",
    "        truncated = False # gymnasium's step returns terminated and truncated\n",
    "\n",
    "        while not done and not truncated: # CartPole-v1 forced to terminates at 500 step.\n",
    "            prob = pi(torch.from_numpy(s).float())\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample()\n",
    "            s_prime, r, done, truncated, info = env.step(a.item())\n",
    "            pi.put_data((r,prob[a]))\n",
    "            s = s_prime\n",
    "            score += r\n",
    "\n",
    "        pi.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "    # Save the model's state dictionary (saved in drive)\n",
    "    torch.save(pi.state_dict(), 'reinforce_cartpole_policy.pth')\n",
    "    print(\"Model saved to reinforce_cartpole_policy.pth\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "#Observations in colab -> \n",
    "#ran it for 2500 episodes, average reward at the end was 350. Best performing agent score 450+ out of 500 and this might need more episodes\n",
    "# but point is proven that a simple gradient function can help learn the agent small and discrete environments like these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55fcd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for video recording\n",
    "#Run thsi in colab to check the downloaded file that has agent playing the game\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import base64\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Instantiate the policy network\n",
    "pi = Policy()\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "try:\n",
    "    pi.load_state_dict(torch.load('reinforce_cartpole_policy.pth'))\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model file 'reinforce_cartpole_policy.pth' not found. Please run the training cell first to save the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "# Create the environment with the RecordVideo wrapper\n",
    "# The video will be saved in a folder named 'videos'\n",
    "env_video = RecordVideo(gym.make('CartPole-v1', render_mode='rgb_array'), video_folder='videos')\n",
    "\n",
    "# Run one episode using the trained policy\n",
    "for i in range(10): #100 episodes to demonstrate\n",
    "  s, info = env_video.reset()\n",
    "  done = False\n",
    "  truncated = False\n",
    "  while not done and not truncated:\n",
    "      # Ensure the model is in evaluation mode\n",
    "      pi.eval()\n",
    "      with torch.no_grad():\n",
    "          prob = pi(torch.from_numpy(s).float())\n",
    "      m = Categorical(prob)\n",
    "      a = m.sample()\n",
    "      s_prime, r, done, truncated, info = env_video.step(a.item())\n",
    "      s = s_prime\n",
    "\n",
    "env_video.close()\n",
    "\n",
    "# Function to display the video in the notebook\n",
    "def display_video(video_path):\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "\n",
    "# Display the last recorded video\n",
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('videos/*.mp4')\n",
    "latest_file = max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
    "\n",
    "if latest_file:\n",
    "    display_video(latest_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
