{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6457a6",
   "metadata": {},
   "source": [
    "#two networks need to be trained in this ALgorithm , typical actor critic framework\n",
    "# first network is policy network which basically outputs the probability distribution of all different set of actions in the env\n",
    "#next is the value network function which will return   value which basically suggests how good the current postion is\n",
    "\n",
    "# Actor Network\n",
    "input_layer → hidden_layers → output_layer(softmax for discrete actions)\n",
    "\n",
    "# Critic Network  \n",
    "input_layer → hidden_layers → output_layer(single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82794adc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#below is sample training loop, need to understand a bit more\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Data Collection\n",
    "    collect_trajectories()\n",
    "    \n",
    "    # 2. Advantage Calculation\n",
    "    compute_advantages_and_returns()\n",
    "    \n",
    "    # 3. Policy Update (multiple times)\n",
    "    for _ in range(policy_updates):\n",
    "        update_actor()\n",
    "        if kl_divergence > threshold:\n",
    "            break  # Early stopping\n",
    "    \n",
    "    # 4. Value Function Update\n",
    "    for _ in range(value_updates):\n",
    "        update_critic()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e713d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
