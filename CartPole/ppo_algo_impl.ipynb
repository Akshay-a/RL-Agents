{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94258fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#this will have extended implementation of basic Policy gradient function and here we will have additional\n",
    "#network function neural network as a critic to judge the actor's actions and improve its odds of balancing the CartPole\n",
    "#this is insipred from open AI article on PPO and below research paper:\n",
    "https://arxiv.org/pdf/1707.06347\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aca965",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#only thing that is special in PPo is that critic sends an advantage function to the actor and then\n",
    "based on the ratio between new policy and old policy ( this is the advantage fxn) we clip the update to a certan range and then run the update\n",
    "this helps in avoiding too much drastic change in network and kind of adds a slow learning ensuring it learns better in time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1449e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20 # Number of steps to collect before training\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "\n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float).to(device), torch.tensor(a_lst).to(device), \\\n",
    "                                          torch.tensor(r_lst).to(device), torch.tensor(s_prime_lst, dtype=torch.float).to(device), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float).to(device), torch.tensor(prob_a_lst).to(device)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "\n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().cpu().numpy() # Move delta to CPU for numpy conversion\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float).to(device) # Move advantage back to GPU\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = PPO().to(device) # Move model to device\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(2500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon): #20 max as 500 is taking some time\n",
    "                prob = model.pi(torch.from_numpy(s).float().to(device)) # Move input to device\n",
    "                m = Categorical(prob.cpu()) # Moved probabilities to CPU for Categorical sampling, \n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done)) # Data is collected as numpy/python\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), 'ppo_cartpole_policy.pth')\n",
    "    print(\"PPO Model saved to ppo_cartpole_policy.pth\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0193c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import base64\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Instantiate the PPO model\n",
    "model = PPO().to(device)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "try:\n",
    "    model.load_state_dict(torch.load('ppo_cartpole_policy.pth', map_location=device))\n",
    "    print(\"PPO Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: PPO model file 'ppo_cartpole_policy.pth' not found. Please run the PPO training cell first to save the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the PPO model: {e}\")\n",
    "\n",
    "# Created the environment with the RecordVideo wrapper\n",
    "# The video will be saved in a folder named 'videos'\n",
    "env_video = RecordVideo(gym.make('CartPole-v1', render_mode='rgb_array'), video_folder='ppo_videos', episode_trigger=lambda x: x % 1 == 0) # Record every episode\n",
    "\n",
    "\n",
    "num_episodes_to_record = 5 \n",
    "for i in range(num_episodes_to_record):\n",
    "    s, info = env_video.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not done and not truncated:\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prob = model.pi(torch.from_numpy(s).float().to(device)) # Move input to device\n",
    "        m = Categorical(prob.cpu()) # Move probabilities to CPU for Categorical sampling\n",
    "        a = m.sample().item()\n",
    "        s_prime, r, done, truncated, info = env_video.step(a)\n",
    "        s = s_prime\n",
    "\n",
    "env_video.close()\n",
    "\n",
    "# Function to display the video in the notebook\n",
    "def display_video(video_path):\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "\n",
    "# Display the last recorded video\n",
    "# The RecordVideo wrapper saves videos with a specific naming convention (e.g., 'rl-video-episode-0.mp4')\n",
    "# We can find the latest video file in the specified video folder\n",
    "video_folder = 'ppo_videos'\n",
    "list_of_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
    "latest_file = max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
    "\n",
    "if latest_file:\n",
    "    print(f\"Displaying video: {latest_file}\")\n",
    "    display_video(latest_file)\n",
    "else:\n",
    "    print(\"No video found in the 'ppo_videos' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cf4c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#This cell compares the performance of Reinforce Algorithm V/s PPO\n",
    "#Above trained models weights are loaded in the colab notebook and run.\n",
    "\"\"\"\n",
    "Observations:\n",
    "even though PPo waas trained for 250 episodes it showed more maturity and gave better Results\n",
    "this will likely outperform more if trained with 10K episodes.\n",
    "TODO - THink about Standard deviation and see how we can reduce it in PPO \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Sample Results\n",
    "--- Evaluation Results ---\n",
    "\n",
    "Algorithm: REINFORCE\n",
    "--------------------\n",
    "Number of Episodes: 100\n",
    "Average Reward: 21.83\n",
    "Reward Standard Deviation: 11.41\n",
    "Maximum Reward: 71.0\n",
    "Minimum Reward: 9.0\n",
    "Success Rate (>= 475): 0.00%\n",
    "Average Episode Length: 21.83\n",
    "\n",
    "Algorithm: PPO\n",
    "------------\n",
    "Number of Episodes: 100\n",
    "Average Reward: 194.49\n",
    "Reward Standard Deviation: 30.22\n",
    "Maximum Reward: 298.0\n",
    "Minimum Reward: 140.0\n",
    "Success Rate (>= 475): 0.00%\n",
    "Average Episode Length: 194.49\n",
    "\n",
    "--- Comparison Summary ---\n",
    "PPO achieved a higher average reward than REINFORCE.\n",
    "REINFORCE exhibited more consistent performance (lower standard deviation).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "#these classes need to be defined if they are running in diff notebook\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Trained Models\n",
    "reinforce_policy = Policy().to(device)\n",
    "ppo_model = PPO().to(device)\n",
    "\n",
    "try:\n",
    "    reinforce_policy.load_state_dict(torch.load('reinforce_cartpole_policy.pth', map_location=device))\n",
    "    print(\"REINFORCE model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: REINFORCE model file 'reinforce_cartpole_policy.pth' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the REINFORCE model: {e}\")\n",
    "\n",
    "try:\n",
    "    ppo_model.load_state_dict(torch.load('ppo_cartpole_policy.pth', map_location=device))\n",
    "    print(\"PPO model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: PPO model file 'ppo_cartpole_policy.pth' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the PPO model: {e}\")\n",
    "\n",
    "reinforce_policy.eval()\n",
    "ppo_model.eval()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "# Run Evaluation Episodes and Collect Metrics\n",
    "num_eval_episodes = 100\n",
    "reinforce_rewards = []\n",
    "ppo_rewards = []\n",
    "\n",
    "print(\"\\nEvaluating REINFORCE Policy...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_eval_episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            prob = reinforce_policy(torch.from_numpy(s).float().to(device))\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "            s = s_prime\n",
    "            episode_reward += r\n",
    "        reinforce_rewards.append(episode_reward)\n",
    "\n",
    "print(\"Evaluating PPO Policy...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_eval_episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            prob = ppo_model.pi(torch.from_numpy(s).float().to(device))\n",
    "            m = Categorical(prob.cpu()) # Sample on CPU as Categorical is CPU-based\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "            s = s_prime\n",
    "            episode_reward += r\n",
    "        ppo_rewards.append(episode_reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calculate Metrics\n",
    "reinforce_avg_reward = np.mean(reinforce_rewards)\n",
    "reinforce_std_reward = np.std(reinforce_rewards)\n",
    "reinforce_max_reward = np.max(reinforce_rewards)\n",
    "reinforce_min_reward = np.min(reinforce_rewards)\n",
    "reinforce_success_rate = np.sum(np.array(reinforce_rewards) >= 475) / num_eval_episodes * 100\n",
    "reinforce_avg_episode_length = reinforce_avg_reward # For CartPole, reward is 1 per step\n",
    "\n",
    "ppo_avg_reward = np.mean(ppo_rewards)\n",
    "ppo_std_reward = np.std(ppo_rewards)\n",
    "ppo_max_reward = np.max(ppo_rewards)\n",
    "ppo_min_reward = np.min(ppo_rewards)\n",
    "ppo_success_rate = np.sum(np.array(ppo_rewards) >= 475) / num_eval_episodes * 100\n",
    "ppo_avg_episode_length = ppo_avg_reward # For CartPole, reward is 1 per step\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "\n",
    "print(\"\\nAlgorithm: REINFORCE\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Number of Episodes: {num_eval_episodes}\")\n",
    "print(f\"Average Reward: {reinforce_avg_reward:.2f}\")\n",
    "print(f\"Reward Standard Deviation: {reinforce_std_reward:.2f}\")\n",
    "print(f\"Maximum Reward: {reinforce_max_reward}\")\n",
    "print(f\"Minimum Reward: {reinforce_min_reward}\")\n",
    "print(f\"Success Rate (>= 475): {reinforce_success_rate:.2f}%\")\n",
    "print(f\"Average Episode Length: {reinforce_avg_episode_length:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nAlgorithm: PPO\")\n",
    "print(\"------------\")\n",
    "print(f\"Number of Episodes: {num_eval_episodes}\")\n",
    "print(f\"Average Reward: {ppo_avg_reward:.2f}\")\n",
    "print(f\"Reward Standard Deviation: {ppo_std_reward:.2f}\")\n",
    "print(f\"Maximum Reward: {ppo_max_reward}\")\n",
    "print(f\"Minimum Reward: {ppo_min_reward}\")\n",
    "print(f\"Success Rate (>= 475): {ppo_success_rate:.2f}%\")\n",
    "print(f\"Average Episode Length: {ppo_avg_episode_length:.2f}\")\n",
    "\n",
    "print(\"\\n--- Comparison Summary ---\")\n",
    "if ppo_avg_reward > reinforce_avg_reward:\n",
    "    print(\"PPO achieved a higher average reward than REINFORCE.\")\n",
    "elif reinforce_avg_reward > ppo_avg_reward:\n",
    "    print(\"REINFORCE achieved a higher average reward than PPO.\")\n",
    "else:\n",
    "    print(\"Both algorithms achieved similar average rewards.\")\n",
    "\n",
    "#check for below conditoins for diff hyperparamters (#TODO)\n",
    "#if ppo_std_reward < reinforce_std_reward:\n",
    "  \n",
    "\n",
    "#if ppo_success_rate > reinforce_success_rate:\n",
    "#    print(\"PPO was more successful in reaching the 'solved' criteria.\")\n",
    "#elif reinforce_success_rate > ppo_success_rate:\n",
    "#    print(\"REINFORCE was more successful in reaching the 'solved' criteria.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
