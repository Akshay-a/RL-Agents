{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94258fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#this will have extended implementation of basic Policy gradient function and here we will have additional\n",
    "#network function neural network as a critic to judge the actor's actions and improve its odds of balancing the CartPole\n",
    "#this is insipred from open AI article on PPO and below research paper:\n",
    "https://arxiv.org/pdf/1707.06347\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aca965",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#only thing that is special in PPo is that critic sends an advantage function to the actor and then\n",
    "based on the ratio between new policy and old policy ( this is the advantage fxn) we clip the update to a certan range and then run the update\n",
    "this helps in avoiding too much drastic change in network and kind of adds a slow learning ensuring it learns better in time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1449e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20 # Number of steps to collect before training\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "\n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float).to(device), torch.tensor(a_lst).to(device), \\\n",
    "                                          torch.tensor(r_lst).to(device), torch.tensor(s_prime_lst, dtype=torch.float).to(device), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float).to(device), torch.tensor(prob_a_lst).to(device)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "\n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().cpu().numpy() # Move delta to CPU for numpy conversion\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float).to(device) # Move advantage back to GPU\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = PPO().to(device) # Move model to device\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(2500):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon): #20 max as 500 is taking some time\n",
    "                prob = model.pi(torch.from_numpy(s).float().to(device)) # Move input to device\n",
    "                m = Categorical(prob.cpu()) # Moved probabilities to CPU for Categorical sampling, \n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done)) # Data is collected as numpy/python\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), 'ppo_cartpole_policy.pth')\n",
    "    print(\"PPO Model saved to ppo_cartpole_policy.pth\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0193c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import base64\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Instantiate the PPO model\n",
    "model = PPO().to(device)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "try:\n",
    "    model.load_state_dict(torch.load('ppo_cartpole_policy.pth', map_location=device))\n",
    "    print(\"PPO Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: PPO model file 'ppo_cartpole_policy.pth' not found. Please run the PPO training cell first to save the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the PPO model: {e}\")\n",
    "\n",
    "# Created the environment with the RecordVideo wrapper\n",
    "# The video will be saved in a folder named 'videos'\n",
    "env_video = RecordVideo(gym.make('CartPole-v1', render_mode='rgb_array'), video_folder='ppo_videos', episode_trigger=lambda x: x % 1 == 0) # Record every episode\n",
    "\n",
    "\n",
    "num_episodes_to_record = 5 \n",
    "for i in range(num_episodes_to_record):\n",
    "    s, info = env_video.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not done and not truncated:\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prob = model.pi(torch.from_numpy(s).float().to(device)) # Move input to device\n",
    "        m = Categorical(prob.cpu()) # Move probabilities to CPU for Categorical sampling\n",
    "        a = m.sample().item()\n",
    "        s_prime, r, done, truncated, info = env_video.step(a)\n",
    "        s = s_prime\n",
    "\n",
    "env_video.close()\n",
    "\n",
    "# Function to display the video in the notebook\n",
    "def display_video(video_path):\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "\n",
    "# Display the last recorded video\n",
    "# The RecordVideo wrapper saves videos with a specific naming convention (e.g., 'rl-video-episode-0.mp4')\n",
    "# We can find the latest video file in the specified video folder\n",
    "video_folder = 'ppo_videos'\n",
    "list_of_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
    "latest_file = max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
    "\n",
    "if latest_file:\n",
    "    print(f\"Displaying video: {latest_file}\")\n",
    "    display_video(latest_file)\n",
    "else:\n",
    "    print(\"No video found in the 'ppo_videos' folder.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
