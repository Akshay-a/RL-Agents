{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train reward model\n",
    "\n",
    "Train a model to score how appropriate the strictness level is for each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayapsingi/Projects/RL-Agents/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Reward model class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import wandb\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "\n",
    "References:\n",
    "- Combining regression and BT: [HelpSteer2-Preference](https://arxiv.org/html/2410.01257v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model for reward base from: /Users/akshayapsingi/Projects/RL-Agents/FineTuneLLms/StrictBot/strictbot_sft_model\n",
      "No config.json found; detected adapters. Loading base model from local cache and attaching adapters...\n",
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 278/278 [00:00<00:00, 8281.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer local SFT directory saved by Step 1\n",
    "primary = Path(\"./strictbot_sft_model\")\n",
    "secondary = Path(\"FineTuneLLms/StrictBot/strictbot_sft_model\")\n",
    "\n",
    "if primary.exists():\n",
    "    sft_dir = primary.resolve()\n",
    "elif secondary.exists():\n",
    "    sft_dir = secondary.resolve()\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"SFT model directory not found. Checked: {primary.resolve()} and {secondary.resolve()}\\n\"\n",
    "        \"Run Step 1 first and ensure the model was saved to 'strictbot_sft_model'.\"\n",
    "    )\n",
    "\n",
    "reward_data_path = \"enhanced_reward_model_dataset.json\"\n",
    "\n",
    "print(f\"Loading SFT model for reward base from: {sft_dir}\")\n",
    "\n",
    "sft_dir_path = Path(sft_dir)\n",
    "config_path = sft_dir_path / \"config.json\"\n",
    "adapter_config_path = sft_dir_path / \"adapter_config.json\"\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"  # must match Step 1 base\n",
    "if (sft_dir_path / \"tokenizer_config.json\").exists():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(sft_dir_path), local_files_only=True)\n",
    "else:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_id, local_files_only=True)\n",
    "    except Exception:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "if config_path.exists():\n",
    "    # Full merged model was saved in Step 1\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(str(sft_dir_path), local_files_only=True)\n",
    "elif adapter_config_path.exists():\n",
    "    print(\"No config.json found; detected adapters. Loading base model from local cache and attaching adapters...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_id, local_files_only=True)\n",
    "    except Exception:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "    from peft import PeftModel\n",
    "    base_model = PeftModel.from_pretrained(base_model, str(sft_dir_path))\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"SFT directory does not contain a full model or PEFT adapters.\\n\"\n",
    "        f\"Missing both: {config_path} and {adapter_config_path}.\\n\"\n",
    "        \"Re-run Step 1 and ensure the model is saved (either merged full model or adapters).\"\n",
    "    )\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "with open(reward_data_path, 'r', encoding='utf-8') as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "# Create dataset of prompt+response and label\n",
    "examples = []\n",
    "for item in raw:\n",
    "    text = f\"<|user|> {item['input']} <|end|>\\n<|assistant|> {item['output']} <|end|>\"\n",
    "    examples.append({\n",
    "        \"text\": text,\n",
    "        \"label\": float(item[\"strictness_score\"])\n",
    "    })\n",
    "\n",
    "hf_ds = Dataset.from_list(examples)\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "hf_ds = hf_ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "hf_ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Reward Head (Regression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM Train Epoch 1/2: 100%|██████████| 32/32 [01:37<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_mse=0.0378 val_mse=0.0076\n",
      "Saved best reward model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM Train Epoch 2/2: 100%|██████████| 32/32 [01:39<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_mse=0.0093 val_mse=0.0048\n",
      "Saved best reward model.\n",
      "Sample score: 0.24470818042755127\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "hidden_size = getattr(base_model.config, \"hidden_size\", None) or getattr(base_model.config, \"n_embd\", 768)\n",
    "\n",
    "class RMHead(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 512), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_lm: AutoModelForCausalLM, in_features: int):\n",
    "        super().__init__()\n",
    "        self.base = base_lm\n",
    "        self.head = RMHead(in_features)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        if attention_mask is None:\n",
    "            seq_len = input_ids.new_full((input_ids.size(0),), input_ids.size(1)-1)\n",
    "        else:\n",
    "            seq_len = attention_mask.sum(dim=1) - 1\n",
    "        pooled = last_hidden[torch.arange(last_hidden.size(0)), seq_len]\n",
    "        scores = self.head(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(scores, labels.float())\n",
    "        return {\"loss\": loss, \"reward_scores\": scores}\n",
    "\n",
    "rm = RewardModel(base_model.to(device), hidden_size).to(device)\n",
    "\n",
    "# Simple train/val split\n",
    "split = int(0.9 * len(hf_ds))\n",
    "train_ds = hf_ds.select(range(split))\n",
    "val_ds = hf_ds.select(range(split, len(hf_ds)))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "optimizer = AdamW(rm.parameters(), lr=1e-4)\n",
    "num_epochs = 2\n",
    "best_val = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rm.train()\n",
    "    total = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"RM Train Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = rm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"]) \n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    avg_train = total / max(1, len(train_loader))\n",
    "\n",
    "    # Eval\n",
    "    rm.eval(); vtotal = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            out = rm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"]) \n",
    "            vtotal += out[\"loss\"].item()\n",
    "    avg_val = vtotal / max(1, len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}: train_mse={avg_train:.4f} val_mse={avg_val:.4f}\")\n",
    "    if avg_val < best_val:\n",
    "        best_val = avg_val\n",
    "        os.makedirs(\"strictbot_reward_model\", exist_ok=True)\n",
    "        torch.save(rm.state_dict(), os.path.join(\"strictbot_reward_model\", \"rm.pt\"))\n",
    "        with open(os.path.join(\"strictbot_reward_model\", \"meta.json\"), \"w\") as f:\n",
    "            f.write(json.dumps({\"base\": str(sft_dir), \"hidden_size\": hidden_size}, indent=2))\n",
    "        tokenizer.save_pretrained(\"strictbot_reward_model\")\n",
    "        print(\"Saved best reward model.\")\n",
    "\n",
    "# Quick check on a sample\n",
    "sample = hf_ds[0]\n",
    "with torch.no_grad():\n",
    "    out = rm(input_ids=sample[\"input_ids\"].unsqueeze(0).to(device), attention_mask=sample[\"attention_mask\"].unsqueeze(0).to(device))\n",
    "print(\"Sample score:\", out[\"reward_scores\"].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
