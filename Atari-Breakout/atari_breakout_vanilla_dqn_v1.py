# -*- coding: utf-8 -*-
"""Atari-breakout-Vanilla-DQN-V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18QIowE6SRUPrry4jrJTufvf86bN5xykr
"""

!pip install atari_py
!pip install gym[atari,accept-rom-license]

import gym
import numpy as np
import random
import math # to use epsilon decay
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from collections import deque
from typing import Tuple, Any
import warnings
import cv2
warnings.filterwarnings('ignore')
np.bool = np.bool_
np.bool8 = np.bool_

def create_basic_env():
    """Create basic Atari Breakout environment"""
    env = gym.make('ALE/Breakout-v5', render_mode='rgb_array')
    print(f"Environment created: {env.spec.id}")
    print(f"Action space: {env.action_space}")
    print(f"Observation space: {env.observation_space}")
    return env


def preprocess_frame(frame: np.ndarray) -> np.ndarray:
    """
    Preprocess a single frame:
    1. Convert to grayscale
    2. Resize to 84x84
    3. Normalize to [0, 1]
    """
    # Convert RGB to grayscale using luminance formula
    gray = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140])

    # Resize to 84x84
    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)

    # Normalize to [0, 1]
    normalized = resized.astype(np.float32) / 255.0

    return normalized


class FrameStack:

    #Maintains a deque of the last n frames. basically to store temporal information
    def __init__(self, num_frames: int = 4):
        self.num_frames = num_frames
        self.frames = deque(maxlen=num_frames)

    def reset(self, frame: np.ndarray):
        """Reset the stack with the initial frame"""
        # Fill the deque with the same frame
        for _ in range(self.num_frames):
            self.frames.append(frame)

    def add_frame(self, frame: np.ndarray):
        """Add a new frame to the stack"""
        self.frames.append(frame)

    def get_stacked_frames(self) -> np.ndarray:
        """Get the current stacked frames as a numpy array"""
        return np.array(list(self.frames))

class AtariPreprocessor:
    """
    Complete Atari environment preprocessor with all required features:
    - Frame preprocessing (grayscale, resize, normalize)
    - Frame skipping (repeat actions for multiple frames)
    - Frame stacking (temporal information)
    - Life-based episode termination
    """

    def __init__(self, env_name: str = 'ALE/Breakout-v5',
                 frame_skip: int = 4,
                 num_stack: int = 4):
        self.env = gym.make(env_name, render_mode='rgb_array')
        self.frame_skip = frame_skip
        self.num_stack = num_stack
        self.frame_stack = FrameStack(num_stack)

        # Track lives for episode termination
        self.lives = 0
        self.was_real_done = True

        print(f"Environment initialized:")
        print(f"  - Frame skip: {frame_skip}")
        print(f"  - Frame stack: {num_stack}")
        print(f"  - Action space: {self.env.action_space}")
        print(f"  - Original observation space: {self.env.observation_space}")
        print(f"  - Processed observation shape: ({num_stack}, 84, 84)")

    def reset(self) -> Tuple[np.ndarray, dict]:
        """Reset the environment and return initial stacked frames"""
        if self.was_real_done:
            obs, info = self.env.reset()
        else:
            # If the episode ended due to life loss, don't reset the environment
            obs, _, _, _, info = self.env.step(0)  # NOOP action

        self.was_real_done = False
        self.lives = info.get('lives', 0)

        # Process and stack initial frame
        processed_frame = preprocess_frame(obs)
        self.frame_stack.reset(processed_frame)

        return self.frame_stack.get_stacked_frames(), info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """Execute action with frame skipping and return processed observation"""
        total_reward = 0.0
        done = False

        # Execute action for frame_skip frames
        for _ in range(self.frame_skip):
            obs, reward, terminated, truncated, info = self.env.step(action)
            total_reward += reward

            # Check for episode termination
            if terminated or truncated:
                done = True
                break

        # Check for life loss (treat as episode end for training)
        lives = info.get('lives', 0)
        if lives < self.lives and lives > 0:
            done = True

        if terminated or truncated:
            self.was_real_done = True

        self.lives = lives

        # Process and stack the frame
        processed_frame = preprocess_frame(obs)
        self.frame_stack.add_frame(processed_frame)

        return (self.frame_stack.get_stacked_frames(),
                total_reward,
                done,
                terminated or truncated,
                info)

    def close(self):
        """Close the environment"""
        self.env.close()

    @property
    def action_space(self):
        return self.env.action_space

    @property
    def observation_space(self):
        # Return the shape of stacked frames
        return gym.spaces.Box(
            low=0.0, high=1.0,
            shape=(self.num_stack, 84, 84),
            dtype=np.float32
        )

def test_basic_environment():
    """Test basic environment functionality"""
    print("=" * 50)
    print("TESTING BASIC ENVIRONMENT")
    print("=" * 50)

    # Test basic environment
    basic_env = create_basic_env()
    obs, info = basic_env.reset()
    print(f"Initial observation shape: {obs.shape}")
    print(f"Initial observation dtype: {obs.dtype}")
    print(f"Initial observation range: [{obs.min()}, {obs.max()}]")

    # Test a few random actions
    for i in range(3):
        print(f"basic action : {basic_env.action_space.sample()}")
        action = basic_env.action_space.sample()
        obs, reward, terminated, truncated, info = basic_env.step(action)
        print(f"Step {i+1}: Action={action}, Reward={reward}, Done={terminated or truncated}")

    basic_env.close()


def test_preprocessing():
    """Test frame preprocessing"""
    print("=" * 50)
    print("TESTING FRAME PREPROCESSING")
    print("=" * 50)

    env = create_basic_env()
    obs, _ = env.reset()

    # Test preprocessing
    processed = preprocess_frame(obs)
    print(f"Original shape: {obs.shape}")
    print(f"Processed shape: {processed.shape}")
    print(f"Original range: [{obs.min()}, {obs.max()}]")
    print(f"Processed range: [{processed.min():.3f}, {processed.max():.3f}]")

    env.close()


def test_frame_stacking():
    """Test frame stacking functionality"""
    print("=" * 50)
    print("TESTING FRAME STACKING")
    print("=" * 50)

    env = create_basic_env()
    obs, _ = env.reset()

    frame_stack = FrameStack(num_frames=4)
    processed_frame = preprocess_frame(obs)
    frame_stack.reset(processed_frame)

    print(f"Stacked frames shape: {frame_stack.get_stacked_frames().shape}")
    print(f"Individual frame shape: {processed_frame.shape}")

    # Simulate a few steps
    for i in range(3):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        processed_frame = preprocess_frame(obs)
        frame_stack.add_frame(processed_frame)
        print(f"Step {i+1}: Stacked frames shape: {frame_stack.get_stacked_frames().shape}")

    env.close()


def test_complete_environment():
    """Test the complete preprocessed environment"""
    print("=" * 50)
    print("TESTING COMPLETE ENVIRONMENT WRAPPER")
    print("=" * 50)

    env = AtariPreprocessor()

    # Reset and get initial observation
    obs, info = env.reset()
    print(f"Initial observation shape: {obs.shape}")
    print(f"Initial observation dtype: {obs.dtype}")
    print(f"Initial observation range: [{obs.min():.3f}, {obs.max():.3f}]")
    print(f"Initial lives: {info.get('lives', 'N/A')}")

    # Test a few steps
    print("\nTesting environment steps:")
    total_reward = 0
    step_count = 0

    for i in range(10):
        action = env.action_space.sample()  # Random action
        obs, reward, done, terminated, info = env.step(action)
        total_reward += reward
        step_count += 1

        print(f"Step {i+1}: Action={action}, Reward={reward}, Done={done}, Lives={info.get('lives', 'N/A')}")

        if done:
            print(f"Episode finished! Total reward: {total_reward}, Steps: {step_count}")
            obs, info = env.reset()
            total_reward = 0
            step_count = 0
            print(f"Environment reset. New lives: {info.get('lives', 'N/A')}")

    env.close()


def test_performance():
    """Test environment performance"""
    print("=" * 50)
    print("TESTING ENVIRONMENT PERFORMANCE")
    print("=" * 50)

    import time

    env = AtariPreprocessor()
    obs, _ = env.reset()

    # Time 1000 steps
    start_time = time.time()
    num_steps = 1000

    for i in range(num_steps):
        action = env.action_space.sample()
        obs, reward, done, terminated, info = env.step(action)

        if done:
            obs, _ = env.reset()

    end_time = time.time()
    steps_per_second = num_steps / (end_time - start_time)

    print(f"Performance Test Results:")
    print(f"  - {num_steps} steps completed in {end_time - start_time:.2f} seconds")
    print(f"  - Performance: {steps_per_second:.1f} steps/second")
    print(f"  - Frame processing overhead: {(1/steps_per_second)*1000:.2f}ms per step")

    env.close()


if __name__ == "__main__":
    # Run all tests
    test_basic_environment()
    test_preprocessing()
    test_frame_stacking()
    test_complete_environment()
    test_performance()

    print("\n" + "=" * 50)
    print("ALL TESTS COMPLETED SUCCESSFULLY!")
    print("Components 1 & 2 are ready for integration with DQN agent.")

import torch
import torch.nn as nn
import torch.nn.functional as F

class DQN(nn.Module):
    #Ip: (batch_size, 4, 84, 84) - 4 stacked grayscale frames , Op: (batch_size, n_actions) - Q-values for each action
    def __init__(self, n_actions=4):
        super(DQN, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)

        # Calculate the size of flattened features
        # Input: (4, 84, 84)
        # After conv1: (32, 20, 20)
        # After conv2: (64, 9, 9)
        # After conv3: (64, 7, 7)
        self.flattened_size = 64 * 7 * 7  # 3136

        # Fully connected layers
        self.fc1 = nn.Linear(self.flattened_size, 512)
        self.fc2 = nn.Linear(512, n_actions)

        # Initialize weights
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize network weights using Xavier initialization"""
        for module in self.modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, x):
        """Forward pass through the network"""
        # Ensure input is float32 and normalized
        if x.dtype != torch.float32:
            x = x.float()

        # Convolutional layers with ReLU activation
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # Flatten for fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

# Test the network
def test_dqn_network():
    """Test DQN network with sample input"""
    print("Testing DQN Network...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create network
    dqn = DQN(n_actions=4).to(device)

    # Test with sample input
    batch_size = 32
    sample_input = torch.randn(batch_size, 4, 84, 84).to(device)

    with torch.no_grad():
        output = dqn(sample_input)

    print(f"Input shape: {sample_input.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Network parameters: {sum(p.numel() for p in dqn.parameters()):,}")
    print("DQN Network test passed! ")

test_dqn_network()

from collections import namedtuple, deque
import random

# Define transition tuple
Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])

class ReplayBuffer:
    """
    Experience Replay Buffer for storing and sampling transitions
    Uses efficient circular buffer with random sampling
    """

    def __init__(self, capacity=1000000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        """Add a transition to the buffer"""
        # Convert numpy arrays to tensors for efficiency
        if isinstance(state, np.ndarray):
            state = torch.from_numpy(state).float()
        if isinstance(next_state, np.ndarray):
            next_state = torch.from_numpy(next_state).float()

        transition = Transition(state, action, reward, next_state, done)
        self.buffer.append(transition)

    def sample(self, batch_size):
        """Sample a batch of transitions"""
        batch = random.sample(self.buffer, batch_size)

        # Unpack batch
        states = torch.stack([t.state for t in batch])
        actions = torch.tensor([t.action for t in batch], dtype=torch.long)
        rewards = torch.tensor([t.reward for t in batch], dtype=torch.float32)
        next_states = torch.stack([t.next_state for t in batch])
        dones = torch.tensor([t.done for t in batch], dtype=torch.bool)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)

    def can_sample(self, batch_size):
        """Check if we can sample a batch"""
        return len(self.buffer) >= batch_size

# Test the replay buffer
def test_replay_buffer():
    """Test replay buffer functionality"""
    print("Testing Replay Buffer...")

    buffer = ReplayBuffer(capacity=1000)

    # Add some sample transitions
    for i in range(100):
        state = np.random.rand(4, 84, 84).astype(np.float32)
        action = random.randint(0, 3)
        reward = random.uniform(-1, 1)
        next_state = np.random.rand(4, 84, 84).astype(np.float32)
        done = random.choice([True, False])

        buffer.push(state, action, reward, next_state, done)

    print(f"Buffer size: {len(buffer)}")
    print(f"Can sample batch of 32: {buffer.can_sample(32)}")

    # Test sampling
    if buffer.can_sample(32):
        states, actions, rewards, next_states, dones = buffer.sample(32)
        print(f"Sampled batch shapes:")
        print(f"  States: {states.shape}")
        print(f"  Actions: {actions.shape}")
        print(f"  Rewards: {rewards.shape}")
        print(f"  Next states: {next_states.shape}")
        print(f"  Dones: {dones.shape}")

    print("Replay Buffer test passed! ✅")

test_replay_buffer()

import torch.optim as optim
import copy

class DQNAgent:
    """
    DQN Agent with epsilon-greedy policy, target network, and Q-learning updates
    """

    def __init__(self, n_actions=4, lr=0.00025, gamma=0.99, epsilon_start=1.0,
                 epsilon_end=0.01, epsilon_decay_steps=1000000,
                 target_update_frequency=500, device=None):

        self.n_actions = n_actions
        self.gamma = gamma
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay_steps = epsilon_decay_steps
        self.target_update_frequency = target_update_frequency

        # Set device
        self.device = device if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"DQN Agent using device: {self.device}")

        # Networks
        self.q_network = DQN(n_actions).to(self.device)
        self.target_network = DQN(n_actions).to(self.device)

        # Initialize target network with same weights
        self.update_target_network()

        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        # Training state
        self.steps_done = 0
        self.epsilon = epsilon_start

        print(f"DQN Agent initialized with {sum(p.numel() for p in self.q_network.parameters()):,} parameters")

    def get_epsilon(self):
        """Get current epsilon value with linear decay"""
        if self.steps_done < self.epsilon_decay_steps:
            self.epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \
                          (self.steps_done / self.epsilon_decay_steps)
        else:
            self.epsilon = self.epsilon_end
        return self.epsilon

    def select_action(self, state, training=True):
        """Select action using epsilon-greedy policy"""
        if training:
            epsilon = self.get_epsilon()
        else:
            epsilon = 0.001  # Small epsilon for evaluation

        if random.random() > epsilon:
            # Exploit: choose best action
            with torch.no_grad():
                if isinstance(state, np.ndarray):
                    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
                else:
                    state = state.unsqueeze(0).to(self.device)

                q_values = self.q_network(state)
                action = q_values.argmax().item()
        else:
            # Explore: choose random action
            action = random.randrange(self.n_actions)

        if training:
            self.steps_done += 1

        return action

    def update_target_network(self):
        """Hard update of target network"""
        self.target_network.load_state_dict(self.q_network.state_dict())

    def learn(self, batch):
        """Update Q-network using a batch of experiences"""
        states, actions, rewards, next_states, dones = batch

        # Move to device
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)

        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # Next Q values from target network
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # Compute loss (using Huber loss for stability)
        loss = F.huber_loss(current_q_values.squeeze(), target_q_values)

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10.0)

        self.optimizer.step()

        # Update target network if needed
        if self.steps_done % self.target_update_frequency == 0:
            self.update_target_network()

        return loss.item()

    def save(self, filepath):
        """Save model checkpoint"""
        checkpoint = {
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'steps_done': self.steps_done,
            'epsilon': self.epsilon
        }
        torch.save(checkpoint, filepath)

    def load(self, filepath):
        """Load model checkpoint"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.steps_done = checkpoint['steps_done']
        self.epsilon = checkpoint['epsilon']

# Test the agent
def test_dqn_agent():
    """Test DQN agent functionality"""
    print("Testing DQN Agent...")

    agent = DQNAgent()

    # Test action selection
    sample_state = np.random.rand(4, 84, 84).astype(np.float32)
    action = agent.select_action(sample_state)
    print(f"Selected action: {action}")
    print(f"Current epsilon: {agent.get_epsilon():.4f}")

    # Test learning with dummy batch
    buffer = ReplayBuffer(capacity=1000)

    # Add some transitions
    for i in range(100):
        state = np.random.rand(4, 84, 84).astype(np.float32)
        action = random.randint(0, 3)
        reward = random.uniform(-1, 1)
        next_state = np.random.rand(4, 84, 84).astype(np.float32)
        done = random.choice([True, False])
        buffer.push(state, action, reward, next_state, done)

    # Sample and learn
    batch = buffer.sample(32)
    loss = agent.learn(batch)
    print(f"Training loss: {loss:.4f}")

    print("DQN Agent test passed! ✅")

test_dqn_agent()

import time
from collections import deque
import matplotlib.pyplot as plt

def train_dqn(episodes=5000, max_steps_per_episode=10000, batch_size=32,  replay_buffer_capacity=100000,
              min_replay_size=20000, update_frequency=4, eval_frequency=100,
              save_frequency=1000):
    """
    Main training loop for DQN agent
    """
    print("=" * 60)
    print("STARTING DQN TRAINING")
    print("=" * 60)

    # Initialize environment and agent
    env = AtariPreprocessor()
    agent = DQNAgent()
    replay_buffer = ReplayBuffer(capacity=replay_buffer_capacity)

    # Training metrics
    episode_rewards = []
    episode_lengths = []
    losses = []
    recent_rewards = deque(maxlen=100)  # For moving average

    # Training state
    total_steps = 0
    best_score = -float('inf')

    print(f"Training parameters:")
    print(f"  Episodes: {episodes}")
    print(f"  Max steps per episode: {max_steps_per_episode}")
    print(f"  Batch size: {batch_size}")
    print(f"  Min replay size: {min_replay_size}")
    print(f"  Update frequency: {update_frequency}")
    print(f"  Device: {agent.device}")
    print()

    start_time = time.time()

    for episode in range(episodes):
        # Reset environment
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        episode_start_time = time.time()

        for step in range(max_steps_per_episode):
            # Select action
            action = agent.select_action(state, training=True)

            # Take action
            next_state, reward, done, terminated, info = env.step(action)

            # Store transition
            replay_buffer.push(state, action, reward, next_state, done)

            # Update state
            state = next_state
            episode_reward += reward
            episode_steps += 1
            total_steps += 1

            # Train agent
            if (replay_buffer.can_sample(batch_size) and
                len(replay_buffer) > min_replay_size and
                total_steps % update_frequency == 0):

                batch = replay_buffer.sample(batch_size)
                loss = agent.learn(batch)
                losses.append(loss)

            # End episode if done
            if done or terminated:
                break

        # Record episode metrics
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_steps)
        recent_rewards.append(episode_reward)

        # Update best score
        if episode_reward > best_score:
            best_score = episode_reward

        # Print progress
        if episode % 10 == 0 or episode < 10:
            avg_reward = np.mean(recent_rewards)
            epsilon = agent.get_epsilon()
            episode_time = time.time() - episode_start_time

            print(f"Episode {episode:4d} | "
                  f"Reward: {episode_reward:6.1f} | "
                  f"Avg: {avg_reward:6.1f} | "
                  f"Best: {best_score:6.1f} | "
                  f"ε: {epsilon:.3f} | "
                  f"Steps: {episode_steps:3d} | "
                  f"Buffer: {len(replay_buffer):6d} | "
                  f"Time: {episode_time:.1f}s")

        # Evaluation
        if episode % eval_frequency == 0 and episode > 0:
            eval_score = evaluate_agent(agent, env, n_episodes=5)
            print(f"  ➤ Evaluation (5 episodes): {eval_score:.1f}")

        # Save model
        if episode % save_frequency == 0 and episode > 0:
            agent.save(f'dqn_checkpoint_episode_{episode}.pth')
            print(f"  ➤ Model saved at episode {episode}")

        # Plot progress
        if episode % 100 == 0 and episode > 0:
            plot_training_progress(episode_rewards, losses)

    # Final evaluation
    print("\n" + "=" * 60)
    print("TRAINING COMPLETED")
    print("=" * 60)

    total_time = time.time() - start_time
    final_eval_score = evaluate_agent(agent, env, n_episodes=10)

    print(f"Total training time: {total_time/3600:.2f} hours")
    print(f"Total episodes: {episodes}")
    print(f"Total steps: {total_steps}")
    print(f"Best training score: {best_score:.1f}")
    print(f"Final evaluation score (10 episodes): {final_eval_score:.1f}")
    print(f"Final epsilon: {agent.get_epsilon():.4f}")

    # Save final model
    agent.save('dqn_final_model.pth')
    print("Final model saved as 'dqn_final_model.pth'")

    # Close environment
    env.close()

    return agent, episode_rewards, losses

def evaluate_agent(agent, env, n_episodes=5):
    """Evaluate agent performance over multiple episodes"""
    total_reward = 0

    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for _ in range(10000):  # Max steps per episode
            action = agent.select_action(state, training=False)
            state, reward, done, terminated, _ = env.step(action)
            episode_reward += reward

            if done or terminated:
                break

        total_reward += episode_reward

    return total_reward / n_episodes

def plot_training_progress(episode_rewards, losses):
    """Plot training progress"""
    plt.figure(figsize=(15, 5))

    # Plot episode rewards
    plt.subplot(1, 3, 1)
    plt.plot(episode_rewards)
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Reward')

    # Plot moving average
    if len(episode_rewards) >= 100:
        moving_avg = [np.mean(episode_rewards[max(0, i-99):i+1]) for i in range(len(episode_rewards))]
        plt.plot(moving_avg, color='red', label='100-episode average')
        plt.legend()

    # Plot losses
    plt.subplot(1, 3, 2)
    if losses:
        plt.plot(losses)
        plt.title('Training Loss')
        plt.xlabel('Update Step')
        plt.ylabel('Loss')

    # Plot recent performance
    plt.subplot(1, 3, 3)
    recent_rewards = episode_rewards[-100:] if len(episode_rewards) >= 100 else episode_rewards
    plt.hist(recent_rewards, bins=20, alpha=0.7)
    plt.title('Recent Reward Distribution')
    plt.xlabel('Reward')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

# Run training (you can adjust parameters as needed)
print("Ready to start training!")
print("To start training, run: agent, rewards, losses = train_dqn(episodes=1000)")

# Quick test with a small number of episodes to verify everything works
def quick_test():
    """Run a quick test to verify the integration works"""
    print("Running quick integration test...")

    # Run training for just 10 episodes
    agent, rewards, losses = train_dqn(episodes=5000, replay_buffer_capacity=100000, min_replay_size=20000)

    print("Quick test completed successfully! ✅")
    print(f"Episode rewards: {rewards}")

    return agent, rewards, losses

# Uncomment to run quick test
agent, rewards, losses = quick_test()